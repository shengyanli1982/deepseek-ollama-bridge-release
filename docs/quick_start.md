# å¿«é€Ÿå…¥é—¨

æ¬¢è¿ä½¿ç”¨ DeepSeek-Ollama Bridgeï¼è¿™æ˜¯ä¸€æ¬¾ä¸“ä¸º DeepSeek æ¨¡å‹é‡èº«æ‰“é€ çš„é«˜æ€§èƒ½ä»£ç†æœåŠ¡ï¼Œå®ƒä¸ºæ‚¨æä¾›ä»¥ä¸‹æ ¸å¿ƒç‰¹æ€§ï¼š

-   æ™ºèƒ½å¤šå±‚ç¼“å­˜ç³»ç»Ÿï¼šç»“åˆå†…å­˜ä¸ç£ç›˜ç¼“å­˜ï¼Œå®ç°æœ€ä¼˜æ€§èƒ½
-   é«˜æ•ˆæµé‡æ§åˆ¶ï¼šåŸºäºä»¤ç‰Œæ¡¶ç®—æ³•ï¼Œç¡®ä¿ç¨³å®šå¯é 
-   æ™ºèƒ½æ€è€ƒæ ‡ç­¾è¿‡æ»¤ï¼šä¸“é—¨ä¼˜åŒ– DeepSeek è’¸é¦æ¨¡å‹çš„è¾“å‡ºè´¨é‡
-   ä¼ä¸šçº§å…±äº«æ¨¡å¼ï¼šæä¾› API Key çš„é›†ä¸­åŒ–æ‰˜ç®¡ä¸ç®¡ç†

#### å¹¿æ³›çš„æ¨¡å‹å…¼å®¹æ€§

DeepSeek-Ollama Bridge é‡‡ç”¨æ ‡å‡†çš„ OpenAI API åè®®ï¼Œæ”¯æŒä¸å„ç±»ä¸»æµå¤§è¯­è¨€æ¨¡å‹å¹³å°æ— ç¼å¯¹æ¥ï¼š

**æœ¬åœ°éƒ¨ç½²å‹å¹³å°**

-   Ollamaï¼šè½»é‡çº§çš„æœ¬åœ°æ¨¡å‹éƒ¨ç½²æ–¹æ¡ˆ
-   LM Studioï¼šä¸€ç«™å¼çš„æ¨¡å‹ç®¡ç†ä¸æ¨ç†å¹³å°

**äº‘æœåŠ¡å‹å¹³å°**

-   Geminiï¼šGoogle æ–°ä¸€ä»£ AI æ¨¡å‹æœåŠ¡
-   Claudeï¼šAnthropic å¼€å‘çš„é«˜æ€§èƒ½åŠ©æ‰‹
-   GPT-4ï¼šOpenAI æœ€æ–°ä¸€ä»£å¤§è¯­è¨€æ¨¡å‹

æ­¤å¤–ï¼Œä»»ä½•æ”¯æŒ OpenAI API åè®®çš„æ¨¡å‹æœåŠ¡éƒ½å¯ä»¥é€šè¿‡æœ¬ä»£ç†è¿›è¡Œè®¿é—®å’Œç®¡ç†ã€‚è¿™ç§å¹¿æ³›çš„å…¼å®¹æ€§è®©æ‚¨èƒ½å¤Ÿçµæ´»é€‰æ‹©æœ€é€‚åˆçš„ AI æ¨¡å‹ï¼ŒåŒæ—¶äº«å—ç»Ÿä¸€çš„æ¥å£ä½“éªŒå’Œæ€§èƒ½ä¼˜åŒ–ã€‚

## å®‰è£…æŒ‡å—

### 1. ä¸‹è½½è½¯ä»¶åŒ…

ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ä¸‹è½½æœ€æ–°ç‰ˆæœ¬ï¼š

```bash
curl -L https://github.com/shengyanli1982/deepseek-ollama-bridge-release/releases/download/v0.1.15/deepseek-ollama-bridge-Linux-x64-v0.1.15.zip -o deepseek-ollama-bridge.zip
```

### 2. è§£å‹æ–‡ä»¶

æ‰§è¡Œè§£å‹å‘½ä»¤ï¼š

```bash
unzip deepseek-ollama-bridge.zip
```

### 3. å¯åŠ¨æœåŠ¡

è¿è¡Œä»¥ä¸‹å‘½ä»¤å¯åŠ¨æœåŠ¡ï¼š

```bash
./deepseek-ollama-bridge-linux-x64
```

### 3.1 æŸ¥çœ‹å¯ç”¨å‚æ•°

ä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹æ‰€æœ‰å¯ç”¨çš„é…ç½®å‚æ•°ï¼š

```bash
./deepseek-ollama-bridge-linux-x64 -h

Usage: deepseek-ollama-bridge.exe [OPTIONS]

Options:
  -l, --listen <LISTEN>
          Network address and port for the bridge service (http) to listen on (format: host:port) [default: 127.0.0.1:3000]
  -o, --ollama <OLLAMA>
          Ollama API endpoint URL for request forwarding (format: http(s)://host:port) [default: http://127.0.0.1:11434]
  -t, --timeout <TIMEOUT>
          Request timeout threshold in seconds for Ollama API communication (range: 1-86400) [default: 300]
  -d, --debug
          Enable detailed debug logging for request tracing and diagnostics
      --enable-cache
          Enable disk-based caching for LLM responses to improve performance and reduce API calls
      --cache-dir <CACHE_DIR>
          Directory path for storing cached LLM responses (format: path/to/cache) [default: ./cache]
      --cache-ttl <CACHE_TTL>
          Cache entry time-to-live in seconds for automatic expiration (range: 30-86400) [default: 300]
      --cache-max-entries <CACHE_MAX_ENTRIES>
          Maximum number of cache entries to maintain in storage (range: 100-1000000) [default: 65535]
      --disable-think-filter
          Disable think-tag filtering for DeepSeek R1 responses (default: enabled)
      --req-rate-limit <REQ_RATE_LIMIT>
          Maximum number of requests to Ollama API per second (0 for unlimited, range: 0-65535) [default: 0]
      --enable-shared-mode
          Enable shared mode for centralized API key management and authentication
      --api-key <API_KEY>
          API key for authentication in shared mode (required when shared mode is enabled)
  -h, --help
          Print help
  -V, --version
          Print version

```

### 3.2 é…ç½®å‚æ•°è¯¦è§£

DeepSeek-Ollama Bridge æä¾›äº†ä¸€å¥—å®Œæ•´çš„é…ç½®å‚æ•°ï¼Œè®©ä½ èƒ½å¤Ÿæ ¹æ®å®é™…éœ€æ±‚çµæ´»è°ƒæ•´æœåŠ¡æ€§èƒ½ã€‚ä»¥ä¸‹æ˜¯è¯¦ç»†è¯´æ˜ï¼š

#### åŸºç¡€é…ç½®å‚æ•°

1. **ç›‘å¬åœ°å€è®¾ç½® (-l, --listen)**

    - å‚æ•°æ ¼å¼ï¼š`host:port`
    - é»˜è®¤é…ç½®ï¼š`127.0.0.1:3000`
    - åŠŸèƒ½è¯´æ˜ï¼šé…ç½®æœåŠ¡ç›‘å¬çš„ç½‘ç»œåœ°å€å’Œç«¯å£ã€‚å¦‚éœ€å…è®¸å¤–éƒ¨è®¿é—®ï¼Œè¯·è®¾ç½®ä¸º `0.0.0.0:3000`
    - ä½¿ç”¨ç¤ºä¾‹ï¼š`--listen 0.0.0.0:3000`

2. **Ollama æœåŠ¡ç«¯ç‚¹ (-o, --ollama)**

    - å‚æ•°æ ¼å¼ï¼š`http(s)://host:port`
    - é»˜è®¤é…ç½®ï¼š`http://127.0.0.1:11434`
    - åŠŸèƒ½è¯´æ˜ï¼šè®¾ç½® Ollama API æœåŠ¡åœ°å€ï¼Œæ”¯æŒ HTTP å’Œ HTTPS åè®®
    - ä½¿ç”¨ç¤ºä¾‹ï¼š`--ollama https://api.deepseek.com`

3. **è¯·æ±‚è¶…æ—¶è®¾ç½® (-t, --timeout)**

    - å¯é€‰èŒƒå›´ï¼š1-86400 ç§’
    - é»˜è®¤é…ç½®ï¼š300 ç§’
    - åŠŸèƒ½è¯´æ˜ï¼šæ§åˆ¶ä¸ Ollama API é€šä¿¡çš„æœ€å¤§ç­‰å¾…æ—¶é—´ï¼Œå»ºè®®æ ¹æ®ç½‘ç»œçŠ¶å†µé€‚å½“è°ƒæ•´
    - ä½¿ç”¨ç¤ºä¾‹ï¼š`--timeout 600`

4. **è°ƒè¯•æ¨¡å¼å¼€å…³ (-d, --debug)**
    - é»˜è®¤çŠ¶æ€ï¼šå…³é—­
    - åŠŸèƒ½è¯´æ˜ï¼šå¼€å¯è¯¦ç»†çš„è°ƒè¯•æ—¥å¿—ï¼Œå¸®åŠ©ä½ è¿½è¸ªè¯·æ±‚æµç¨‹å’Œè¯Šæ–­é—®é¢˜
    - ä½¿ç”¨ç¤ºä¾‹ï¼š`--debug`

#### ç¼“å­˜ç³»ç»Ÿé…ç½®

5. **ç¼“å­˜åŠŸèƒ½å¼€å…³ (--enable-cache)**

    - é»˜è®¤çŠ¶æ€ï¼šå…³é—­
    - åŠŸèƒ½è¯´æ˜ï¼šå¯ç”¨ç£ç›˜ç¼“å­˜æœºåˆ¶ï¼Œæ˜¾è‘—æå‡å“åº”é€Ÿåº¦å¹¶é™ä½ API è°ƒç”¨é¢‘ç‡
    - ä½¿ç”¨ç¤ºä¾‹ï¼š`--enable-cache`

6. **ç¼“å­˜å­˜å‚¨è·¯å¾„ (--cache-dir)**

    - å‚æ•°æ ¼å¼ï¼š`path/to/cache`
    - é»˜è®¤é…ç½®ï¼š`./cache`
    - åŠŸèƒ½è¯´æ˜ï¼šæŒ‡å®šç¼“å­˜æ–‡ä»¶çš„å­˜å‚¨ä½ç½®ï¼Œè¯·ç¡®ä¿ç›®å½•å…·å¤‡è¯»å†™æƒé™
    - ä½¿ç”¨ç¤ºä¾‹ï¼š`--cache-dir /data/llm/cache`

7. **ç¼“å­˜æœ‰æ•ˆæœŸè®¾ç½® (--cache-ttl)**

    - å¯é€‰èŒƒå›´ï¼š30-86400 ç§’
    - é»˜è®¤é…ç½®ï¼š300 ç§’
    - åŠŸèƒ½è¯´æ˜ï¼šè®¾å®šç¼“å­˜æ•°æ®çš„æœ‰æ•ˆæœŸé™ï¼Œè¶…æœŸæ•°æ®å°†è‡ªåŠ¨å¤±æ•ˆ
    - ä½¿ç”¨ç¤ºä¾‹ï¼š`--cache-ttl 3600`

8. **ç¼“å­˜å®¹é‡é™åˆ¶ (--cache-max-entries)**
    - å¯é€‰èŒƒå›´ï¼š100-1000000 æ¡
    - é»˜è®¤é…ç½®ï¼š65535 æ¡
    - åŠŸèƒ½è¯´æ˜ï¼šé™å®šç¼“å­˜æ¡ç›®çš„æœ€å¤§æ•°é‡ï¼Œè¶…å‡ºé™åˆ¶æ—¶å°†å¯ç”¨ LRU æ¸…ç†æœºåˆ¶
    - ä½¿ç”¨ç¤ºä¾‹ï¼š`--cache-max-entries 100000`

#### é«˜çº§åŠŸèƒ½é…ç½®

9. **æ€è€ƒæ ‡ç­¾è¿‡æ»¤å™¨ (--disable-think-filter)**

    - é»˜è®¤çŠ¶æ€ï¼šå¯ç”¨
    - åŠŸèƒ½è¯´æ˜ï¼šæ§åˆ¶æ˜¯å¦è¿‡æ»¤ DeepSeek R1 æ¨¡å‹è¾“å‡ºä¸­çš„æ€è€ƒæ ‡ç­¾
    - ä½¿ç”¨ç¤ºä¾‹ï¼š`--disable-think-filter`

10. **è¯·æ±‚é€Ÿç‡æ§åˆ¶ (--req-rate-limit)**
    - å¯é€‰èŒƒå›´ï¼š0-65535ï¼ˆ0 ä»£è¡¨ä¸é™åˆ¶ï¼‰
    - é»˜è®¤é…ç½®ï¼š0
    - åŠŸèƒ½è¯´æ˜ï¼šæ§åˆ¶æ¯ç§’å‘é€è‡³ Ollama API çš„æœ€å¤§è¯·æ±‚æ•°ï¼Œç”¨äºä¿æŠ¤æœåŠ¡ç¨³å®šæ€§
    - ä½¿ç”¨ç¤ºä¾‹ï¼š`--req-rate-limit 100`

#### å…±äº«æ¨¡å¼é…ç½®

11. **å…±äº«æ¨¡å¼å¼€å…³ (--enable-shared-mode)**

    -   é»˜è®¤çŠ¶æ€ï¼šå…³é—­
    -   åŠŸèƒ½è¯´æ˜ï¼šå¯ç”¨ä¼ä¸šçº§çš„ API å¯†é’¥é›†ä¸­ç®¡ç†åŠŸèƒ½
    -   ä½¿ç”¨ç¤ºä¾‹ï¼š`--enable-shared-mode`

12. **API å¯†é’¥é…ç½® (--api-key)**
    -   å‚æ•°æ ¼å¼ï¼šå­—ç¬¦ä¸²
    -   åŠŸèƒ½è¯´æ˜ï¼šé…ç½®å…±äº«æ¨¡å¼ä¸‹çš„è®¤è¯å¯†é’¥
    -   ä½¿ç”¨ç¤ºä¾‹ï¼š`--api-key your_api_key`

#### ä½¿ç”¨å»ºè®®

-   æ—¶é—´ç±»å‚æ•°å‡é‡‡ç”¨ç§’ä½œä¸ºå•ä½ï¼Œä¾¿äºç»Ÿä¸€ç®¡ç†
-   å¯ç”¨ç¼“å­˜å¯å¤§å¹…æå‡æ€§èƒ½ï¼Œä½†è¯·é¢„ç•™è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´
-   ç”Ÿäº§ç¯å¢ƒå»ºè®®å¼€å¯å…±äº«æ¨¡å¼ï¼Œå®ç° API å¯†é’¥çš„ç»Ÿä¸€ç®¡ç†
-   è°ƒè¯•æ¨¡å¼å»ºè®®ä»…åœ¨å¼€å‘æµ‹è¯•é˜¶æ®µä½¿ç”¨ï¼Œé¿å…æ—¥å¿—è¿‡å¤š
-   è¯·æ±‚é€Ÿç‡é™åˆ¶è¦ä¸æœåŠ¡å™¨æ€§èƒ½å’Œä¸šåŠ¡éœ€æ±‚ç›¸åŒ¹é…

### 3.3 é…ç½®ç¤ºä¾‹

ä»¥ä¸‹æ˜¯ä¸€ä¸ªè®¿é—® DeepSeek å®˜æ–¹æœåŠ¡çš„å®Œæ•´é…ç½®ç¤ºä¾‹ï¼š

```bash
./deepseek-ollama-bridge -l 127.0.0.1:3000 -o https://api.deepseek.com -t 300 --enable-cache --cache-dir ./cache --cache-ttl 30 --enable-shared-mode --api-key your_api_key
```

### 4. æœåŠ¡è°ƒç”¨

**é‡è¦æç¤ºï¼š** è¯·å°†ç¤ºä¾‹ä¸­çš„ `http://127.0.0.1:3000` æ›¿æ¢ä¸ºä½ å®é™…é…ç½®çš„æœåŠ¡åœ°å€å’Œç«¯å£ã€‚

```bash
$ curl -N http://127.0.0.1:3000/v1/chat/completions -H "Content-Type: application/json" -d '{
  "model": "deepseek-r1",
  "messages": [{"role": "user", "content": "your message1"}],
  "stream": true
}'
```

**è¿”å›ç¤ºä¾‹ï¼š**

```bash
data: {"id":"chatcmpl-0","object":"chat.completion.chunk","created":1739943594,"model":"deepseek-r1","system_fingerprint":null,"choices":[{"index":0,"delta":{"role":"assistant","content":"It seems like there might be some confusion in your message. If you're referring to a previous part of a story or a specific request, feel free to clarify or provide more context! For example:\n\n- Are you asking me to continue a story we were working on earlier?  \n- Do you need help brainstorming ideas?  \n- Or is this part of a different task?  \n\nLet me know how I can assist! ğŸ˜Š"},"finish_reason":null}]}

data: {"id":"chatcmpl-0","object":"chat.completion.chunk","created":1739943594,"model":"deepseek-r1","system_fingerprint":null,"choices":[{"index":0,"delta":{"role":"assistant","content":""},"finish_reason":"stop"}]}

data: [DONE]
```

**è¿è¡Œæ—¥å¿—åˆ†æ**

ä»¥ä¸‹æ˜¯ä¸€æ®µå…¸å‹çš„è¿è¡Œæ—¥å¿—ï¼Œä½ å¯ä»¥é€šè¿‡è¿™äº›ä¿¡æ¯æ¥äº†è§£è¯·æ±‚çš„å¤„ç†è¿‡ç¨‹ï¼š

```bash
2025-02-19T05:40:20.145064Z  INFO deepseek_ollama_bridge::handlers: 51: [req-3] Method: POST, Path: /v1/chat/completions, Protocol: HTTP/1.1
2025-02-19T05:40:20.145135Z  INFO deepseek_ollama_bridge::handlers: 108: [req-3] Forwarding request to: https://api.lkeap.cloud.tencent.com/v1/chat/completions
2025-02-19T05:40:20.145167Z DEBUG deepseek_ollama_bridge::handlers: 109: [req-3] Request headers: {"host": "127.0.0.1:3000", "user-agent": "curl/8.10.1", "accept": "*/*", "content-type": "application/json", "content-length": "108"}
2025-02-19T05:40:20.145199Z  INFO deepseek_ollama_bridge::handlers: 114: [req-3] Request body size: 108 bytes
2025-02-19T05:40:20.145225Z DEBUG deepseek_ollama_bridge::handlers: 115: [req-3] Request body content: {
  "model": "deepseek-r1",
  "messages": [{"role": "user", "content": "your message1"}],
  "stream": true
}
2025-02-19T05:40:20.145277Z  INFO deepseek_ollama_bridge::handlers: 128: [req-3] Private mode is enabled, using custom API key for authorization
2025-02-19T05:40:20.145302Z  INFO deepseek_ollama_bridge::handlers: 131: [req-3] Converted request headers, Content-Type: Some("application/json")
2025-02-19T05:40:20.145331Z DEBUG deepseek_ollama_bridge::handlers: 146: [req-3] Streaming response detected
2025-02-19T05:40:20.145365Z  INFO deepseek_ollama_bridge::cache: 309: Cache Manager - Memory cache hit, Key: c65aad7d2feceedf0498615bad07f1f4, Latency: 2.50Âµs
2025-02-19T05:40:20.145395Z  INFO deepseek_ollama_bridge::cache: 522: Cache Manager - Context cache hit with prefix length 1 (stream_mode: true)
2025-02-19T05:40:20.145423Z DEBUG deepseek_ollama_bridge::cache: 523: Cache Manager - Context cache hit with prefixs: [user:your message1]
2025-02-19T05:40:20.145450Z DEBUG deepseek_ollama_bridge::handlers: 304: [req-3] Context cache hit
2025-02-19T05:40:20.145479Z  INFO deepseek_ollama_bridge::response: 49: [req-3] Final response status: 200 OK
2025-02-19T05:40:20.145510Z DEBUG deepseek_ollama_bridge::response: 90: [req-3] Final response headers: {"content-type": "text/event-stream", "connection": "keep-alive", "cache-control": "no-cache", "content-length": "836"}
2025-02-19T05:40:20.145538Z  INFO deepseek_ollama_bridge::response: 91: [req-3] Final response body size: 836 bytes
2025-02-19T05:40:20.145566Z  INFO deepseek_ollama_bridge::handlers: 76: [req-3] LLM request completed successfully, Latency: 502.70Âµs
```

ä»æ—¥å¿—ä¸­å¯ä»¥çœ‹åˆ° `Cache Manager - Context cache hit with prefixs: [user:your message1]` çš„æç¤ºï¼Œè¿™è¡¨æ˜ç³»ç»ŸæˆåŠŸå‘½ä¸­äº†ç¼“å­˜ï¼Œæœ‰æ•ˆæå‡äº†å“åº”é€Ÿåº¦ã€‚
