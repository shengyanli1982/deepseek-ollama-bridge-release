# ğŸš€ DeepSeek-Ollama Bridgeï¼šè®©ä½ çš„ AI å¯¹è¯æ›´å¿«ã€æ›´ç¨³ã€æ›´çœå¿ƒï¼

## ğŸ˜« ä½ æ˜¯å¦é‡åˆ°è¿‡è¿™äº›çƒ¦æ¼ï¼Ÿ

-   DeepSeek æ¨¡å‹æœ¬åœ°éƒ¨ç½²åé‡å¤è®¡ç®—ç›¸åŒé—®é¢˜ï¼Œç®—åŠ›èµ„æºä¸¥é‡æµªè´¹
-   é«˜å¹¶å‘åœºæ™¯ä¸‹ç³»ç»Ÿä¸ç¨³å®šï¼Œå“åº”å»¶è¿Ÿå¤§å¹…æ³¢åŠ¨
-   æ¨¡å‹è¾“å‡ºå¤¹æ‚æ€è€ƒæ ‡ç­¾ï¼Œå½±å“å¯¹è¯ä½“éªŒ
-   æœåŠ¡å™¨èµ„æºå‘Šæ€¥ï¼Œæ€§èƒ½è°ƒä¼˜æ— ä»ä¸‹æ‰‹

## ğŸ¯ è§£å†³æ–¹æ¡ˆæ¥äº†ï¼

DeepSeek-Ollama Bridge æ˜¯ä¸€æ¬¾ä¸“ä¸º DeepSeek æ¨¡å‹æ‰“é€ çš„é«˜æ€§èƒ½ä»£ç†ï¼ˆProxyï¼‰æœåŠ¡ï¼Œè®©æ‚¨çš„ AI åº”ç”¨å¦‚è™æ·»ç¿¼ï¼

### ğŸ æ ¸å¿ƒç‰¹æ€§

#### 1ï¸ æ™ºèƒ½å¤šå±‚ç¼“å­˜ç³»ç»Ÿ

-   é«˜æ€§èƒ½å†…å­˜çƒ­ç‚¹ç¼“å­˜ï¼ˆ1024 æ¡ï¼‰ï¼Œæé€Ÿå“åº”
-   ç£ç›˜æŒä¹…åŒ–å­˜å‚¨ï¼Œä¸å—å†…å­˜å¤§å°é™åˆ¶ï¼Œæ”¯æŒç™¾ä¸‡çº§ç¼“å­˜
-   å¯¹è¯ä¸Šä¸‹æ–‡æ„ŸçŸ¥ï¼Œæ™ºèƒ½åŒ¹é…å†å²åº”ç­”
-   è‡ªåŠ¨åŒ–æ¸…ç†æœºåˆ¶ï¼Œæ— éœ€äººå·¥ç»´æŠ¤
-   çµæ´»çš„ç¼“å­˜å‚æ•°é…ç½®ï¼Œè½»æ¾åº”å¯¹å„ç±»åœºæ™¯
-   æ³¨æ„ï¼šæµå¼è¾“å‡ºæ¨¡å¼ä¸‹ä» v0.1.15 ç‰ˆæœ¬å¼€å§‹æ”¯æŒ

#### 2ï¸ æˆç†Ÿçš„æµé‡æ§åˆ¶

-   ä»¤ç‰Œæ¡¶é™æµä¿æŠ¤ï¼Œé˜²æ­¢ç³»ç»Ÿè¿‡è½½
-   Prometheus æŒ‡æ ‡ç›‘æ§ï¼Œè¿è¡ŒçŠ¶æ€ä¸€ç›®äº†ç„¶

#### 3ï¸ å®¹å™¨éƒ¨ç½²æ”¯æŒ

-   æ”¯æŒ Kubernetes é›†ç¾¤éƒ¨ç½²
-   ä¼˜é›…å¯åœæœºåˆ¶
-   è·¨å¹³å°å…¼å®¹æ€§æ”¯æŒ
-   å®Œæ•´çš„ç›‘æ§æŒ‡æ ‡

#### 4ï¸ æ™ºèƒ½æ€è€ƒæ ‡ç­¾è¿‡æ»¤

-   ä¸“ä¸º DeepSeek è’¸é¦æ¨¡å‹ä¼˜åŒ–çš„æ€è€ƒæ ‡ç­¾è¿‡æ»¤ç³»ç»Ÿ
-   è‡ªåŠ¨è¯†åˆ«å¹¶ç§»é™¤æ¨¡å‹è¾“å‡ºä¸­çš„æ€è€ƒè¿‡ç¨‹æ ‡è®°
-   ä¿æŒè¾“å‡ºå†…å®¹çš„ä¸“ä¸šæ€§å’Œè¿è´¯æ€§
-   é›¶å»¶è¿Ÿå¤„ç†ï¼Œä¸å½±å“æ¨¡å‹å“åº”é€Ÿåº¦

#### 5ï¸ ä¼ä¸šçº§å…±äº«æ¨¡å¼

-   API Key é›†ä¸­æ‰˜ç®¡ä¸ç®¡ç†

### ğŸ’ª ä¸ºä»€ä¹ˆé€‰æ‹© DeepSeek-Ollama Bridge ï¼Ÿ

-   ä¸º DeepSeek æ¨¡å‹ä¼˜åŒ–ï¼ŒåŒæ—¶å…¼å®¹å…¶ä»– OpenAI API è§„èŒƒçš„æ¨¡å‹
-   å¼€ç®±å³ç”¨ï¼Œå¯ä»¥é›¶é…ç½®å¯åŠ¨
-   æ˜¾è‘—æå‡å“åº”é€Ÿåº¦ï¼Œé™ä½è®¡ç®—æˆæœ¬
-   è‡ªåŠ¨è¿‡æ»¤æ€è€ƒæ ‡ç­¾ï¼ˆä¸“é—¨é’ˆå¯¹ DeepSeek è’¸é¦æ¨¡å‹ï¼‰ï¼Œè¾“å‡ºæ›´æ¸…æ™°ä¸“ä¸š

### ğŸ¬ å…¸å‹åº”ç”¨åœºæ™¯

#### 1ï¸ é«˜é¢‘å¯¹è¯åœºæ™¯

-   æ™ºèƒ½å®¢æœç³»ç»Ÿ
-   æ•™è‚²é—®ç­”å¹³å°
-   API é›†æˆæœåŠ¡

#### 2ï¸ èµ„æºå—é™ç¯å¢ƒ

-   ä¸ªäººå¼€å‘ç¯å¢ƒ
-   è¾¹ç¼˜è®¡ç®—è®¾å¤‡
-   å…±äº«è®¡ç®—é›†ç¾¤

#### 3ï¸ ä¼ä¸šçº§åº”ç”¨

-   å¤§è§„æ¨¡ AI æœåŠ¡éƒ¨ç½²
-   å¤šç§Ÿæˆ·å¹¶å‘è®¿é—®
-   æˆæœ¬æ•æ„Ÿå‹ä¸šåŠ¡
-   Key æ‰˜ç®¡

### ğŸ“š æ”¯æŒæ–‡æ¡£

å¯ä»¥é˜…è¯» `docs` ç›®å½•ä¸‹çš„æ–‡æ¡£ï¼Œäº†è§£ DeepSeek-Ollama Bridge çš„è¯¦ç»†ä½¿ç”¨æ–¹æ³•ã€‚

### ğŸ“ å®ç”¨æ–¹æ¡ˆ

å¯ä»¥é˜…è¯» `practical_schemes` ç›®å½•ä¸‹çš„æ–‡æ¡£ï¼Œäº†è§£ DeepSeek-Ollama Bridge çš„å®ç”¨æ–¹æ¡ˆã€‚

### ğŸ“ ç‰ˆæœ¬æ›´æ–°

å¯ä»¥é˜…è¯» `CHANGLOG.md` æ–‡ä»¶ï¼Œäº†è§£ DeepSeek-Ollama Bridge çš„ç‰ˆæœ¬æ›´æ–°å†å²ã€‚

### ğŸ¤– é»˜è®¤æ¥å£

-   **/health** : å¥åº·æ£€æŸ¥æ¥å£
-   **/metrics** : Prometheus æŒ‡æ ‡ç›‘æ§æ¥å£

### ğŸ“Š æ•ˆæœå±•ç¤º

#### 1. ç¼“å­˜æ•ˆæœ (stream=false)

![ç¼“å­˜æ•ˆæœ](./images/cache.png)

#### 2. ç¼“å­˜æ•ˆæœ (stream=true)

![ç¼“å­˜æ•ˆæœ](./images/cache-stream.png)

#### 3. æµé‡æ§åˆ¶

![æµé‡æ§åˆ¶](./images/rate-limit.png)

## ğŸ å¿«é€Ÿå¼€å§‹

åªéœ€ä¸€è¡Œå‘½ä»¤ï¼Œå³å¯å¯åŠ¨ä¼ä¸šçº§ AI åŠ é€ŸæœåŠ¡ï¼š

```bash
deepseek-ollama-bridge --enable-cache --cache-dir ./cache
```

æ›´å¤šé«˜çº§é…ç½®é€‰é¡¹è¯·ä½¿ç”¨ `-h` å‚æ•°æŸ¥çœ‹å¸®åŠ©æ–‡æ¡£ã€‚

_æ³¨ï¼šå®é™…æ€§èƒ½æå‡å› ä½¿ç”¨åœºæ™¯å’Œé…ç½®è€Œå¼‚ã€‚æ¬¢è¿ç•™è¨€åé¦ˆé—®é¢˜å’Œæ”¹è¿›å»ºè®®ã€‚_

## ğŸ’¡ ä»£ç ç¤ºä¾‹

### cURL ç¤ºä¾‹

```bash
curl http://127.0.0.1:3000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer sk-xxx" \
  -d '{
    "model": "deepseek-coder",
    "messages": [
      {
        "role": "user",
        "content": "å†™ä¸€ä¸ªå†’æ³¡æ’åºç®—æ³•"
      }
    ],
    "temperature": 0.7
  }'
```

### Python ç¤ºä¾‹

```python
import openai

# è®¾ç½® API åŸºç¡€åœ°å€ï¼ˆé»˜è®¤ä¸ºæœ¬åœ°æœåŠ¡ï¼‰
openai.api_base = "http://127.0.0.1:3000/v1/chat/completions"
# è®¾ç½®ä¸€ä¸ªå ä½ API Keyï¼ˆæœ¬åœ°æœåŠ¡ä¸æ ¡éªŒï¼‰
openai.api_key = "sk-xxx"

# åŸºç¡€å¯¹è¯ç¤ºä¾‹
def chat_example():
    response = openai.ChatCompletion.create(
        model="deepseek-coder",  # ä½¿ç”¨ DeepSeek Coder æ¨¡å‹
        messages=[
            {"role": "user", "content": "å†™ä¸€ä¸ª Python å¿«é€Ÿæ’åºç®—æ³•"}
        ],
        temperature=0.7
    )
    print(response.choices[0].message.content)

# å¸¦ä¸Šä¸‹æ–‡çš„å¯¹è¯ç¤ºä¾‹
def context_chat_example():
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¼–ç¨‹åŠ©æ‰‹ã€‚"},
        {"role": "user", "content": "æˆ‘æƒ³å®ç°ä¸€ä¸ª REST APIã€‚"},
        {"role": "assistant", "content": "æˆ‘å¯ä»¥å¸®ä½ ä½¿ç”¨ FastAPI æ¡†æ¶å®ç°ã€‚"},
        {"role": "user", "content": "å¥½çš„ï¼Œè¯·ç»™å‡ºå…·ä½“ç¤ºä¾‹ã€‚"}
    ]

    response = openai.ChatCompletion.create(
        model="deepseek-coder",
        messages=messages,
        temperature=0.7
    )

    print(response.choices[0].message.content)

if __name__ == "__main__":
    print("åŸºç¡€å¯¹è¯ç¤ºä¾‹ï¼š")
    chat_example()

    print("\nå¸¦ä¸Šä¸‹æ–‡çš„å¯¹è¯ç¤ºä¾‹ï¼š")
    context_chat_example()
```

### Go ç¤ºä¾‹

```go
package main

import (
    "context"
    "fmt"
    "log"

    openai "github.com/sashabaranov/go-openai"
)

func main() {
    // åˆ›å»ºå®¢æˆ·ç«¯ï¼ˆä½¿ç”¨æœ¬åœ°æœåŠ¡åœ°å€ï¼‰
    client := openai.NewClient("sk-xxx")
    client.BaseURL = "http://127.0.0.1:3000/v1/chat/completions"

    // åˆ›å»ºå¯¹è¯è¯·æ±‚
    req := openai.ChatCompletionRequest{
        Model: "deepseek-coder",
        Messages: []openai.ChatCompletionMessage{
            {
                Role:    openai.ChatMessageRoleUser,
                Content: "ç”¨ Go å®ç°ä¸€ä¸ªç®€å•çš„ HTTP æœåŠ¡å™¨",
            },
        },
        Temperature: 0.7,
    }

    // å‘é€è¯·æ±‚
    resp, err := client.CreateChatCompletion(context.Background(), req)
    if err != nil {
        log.Printf("å¯¹è¯è¯·æ±‚å¤±è´¥: %v\n", err)
        return
    }

    // è¾“å‡ºå“åº”
    fmt.Println(resp.Choices[0].Message.Content)

    // å¸¦ä¸Šä¸‹æ–‡çš„å¯¹è¯ç¤ºä¾‹
    contextReq := openai.ChatCompletionRequest{
        Model: "deepseek-coder",
        Messages: []openai.ChatCompletionMessage{
            {
                Role:    openai.ChatMessageRoleSystem,
                Content: "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ Go å¼€å‘ä¸“å®¶ã€‚",
            },
            {
                Role:    openai.ChatMessageRoleUser,
                Content: "è§£é‡Šä»€ä¹ˆæ˜¯ä¾èµ–æ³¨å…¥",
            },
        },
        Temperature: 0.7,
    }

    contextResp, err := client.CreateChatCompletion(context.Background(), contextReq)
    if err != nil {
        log.Printf("ä¸Šä¸‹æ–‡å¯¹è¯è¯·æ±‚å¤±è´¥: %v\n", err)
        return
    }

    fmt.Println("\nå¸¦ä¸Šä¸‹æ–‡çš„å¯¹è¯å“åº”:")
    fmt.Println(contextResp.Choices[0].Message.Content)
}
```

### NodeJS ç¤ºä¾‹

```javascript
const { Configuration, OpenAIApi } = require("openai");

// é…ç½® OpenAI API
const configuration = new Configuration({
    basePath: "http://127.0.0.1:3000/v1/chat/completions",
    apiKey: "sk-xxx",
});

const openai = new OpenAIApi(configuration);

// åŸºç¡€å¯¹è¯ç¤ºä¾‹
async function basicChatExample() {
    try {
        const response = await openai.createChatCompletion({
            model: "deepseek-coder",
            messages: [{ role: "user", content: "ç”¨ Express å®ç°ä¸€ä¸ª RESTful API" }],
            temperature: 0.7,
        });

        console.log("åŸºç¡€å¯¹è¯å“åº”:", response.data.choices[0].message.content);
    } catch (error) {
        console.error("å¯¹è¯è¯·æ±‚å¤±è´¥:", error.message);
    }
}

// å¸¦ä¸Šä¸‹æ–‡çš„å¯¹è¯ç¤ºä¾‹
async function contextChatExample() {
    try {
        const response = await openai.createChatCompletion({
            model: "deepseek-coder",
            messages: [
                { role: "system", content: "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ Node.js å¼€å‘ä¸“å®¶ã€‚" },
                { role: "user", content: "å¦‚ä½•å®ç°ä¸€ä¸ª WebSocket æœåŠ¡å™¨ï¼Ÿ" },
            ],
            temperature: 0.7,
        });

        console.log("\nå¸¦ä¸Šä¸‹æ–‡çš„å¯¹è¯å“åº”:");
        console.log(response.data.choices[0].message.content);
    } catch (error) {
        console.error("å¯¹è¯è¯·æ±‚å¤±è´¥:", error.message);
    }
}

// æ‰§è¡Œç¤ºä¾‹
async function main() {
    console.log("=== åŸºç¡€å¯¹è¯ç¤ºä¾‹ ===");
    await basicChatExample();

    console.log("\n=== å¸¦ä¸Šä¸‹æ–‡çš„å¯¹è¯ç¤ºä¾‹ ===");
    await contextChatExample();
}

main().catch(console.error);
```
